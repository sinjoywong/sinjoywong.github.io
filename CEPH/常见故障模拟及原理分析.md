# OSD状态表

| 状态——————— | 说明                                                         |
| :---------- | :----------------------------------------------------------- |
| up          | osd启动                                                      |
| down        | osd停止                                                      |
| in          | osd在集群中                                                  |
| out         | osd不在集群中，默认OSD down 超过300s,Ceph会标记为out，会触发重新平衡操作 |
| up & in     | 说明该OSD正常运行，且已经承载至少一个PG的数据。这是一个OSD的标准工作状态 |
| up & out    | 说明该OSD正常运行，但并未承载任何PG，其中也没有数据。一个新的OSD刚刚被加入Ceph集群后，便会处于这一状态。而一个出现故障的OSD被修复后，重新加入Ceph集群时，也是处于这一状态 |
| down & in   | 说明该OSD发生异常，但仍然承载着至少一个PG，其中仍然存储着数据。这种状态下的OSD刚刚被发现存在异常，可能仍能恢复正常，也可能会彻底无法工作 |
| down & out  | 说明该OSD已经彻底发生故障，且已经不再承载任何PG              |

暂时关闭pg重新平衡

在维护操作或解决问题时，不希望在停止一些OSD后，超时的OSD被标记为out后，CRUSH算法自动进行重新平衡操作。需要执行集群关闭out检测命令：

```
ceph osd set noout
```

这样在停止的OSD中的PG会变为降级态。当维护操作完成后，需要先启动停止的OSD，再恢复默认设置：

```
ceph osd unset noout
```

# PG 状态表

正常是active+clean

| 状态         | 描述                                                         |
| :----------- | :----------------------------------------------------------- |
| active       | 活跃状态。ceph将处理到达这个PG的读写请求                     |
| unactive     | 非活跃状态。该PG不能处理读写请求                             |
| clean        | 干净状态。Ceph复制PG内所有对象到设定正确的数目               |
| unclean      | 非干净状态。PG不能从上一个失败中恢复                         |
| down         | 离线状态。有必需数据的副本挂掉，比如对象所在的3个副本的OSD挂掉，所以PG离线 |
| degraded     | 降级状态。ceph有些对象的副本数目没有达到系统设置，一般是因为有OSD挂掉 |
| inconsistent | 不一致态。Ceph 清理和深度清理后检测到PG中的对象在副本存在不一致，例如对象的文件大小不一致或recovery结束后一个对象的副本丢失 |
| peering      | 正在同步状态。PG正在执行同步处理                             |
| recovering   | 正在恢复状态。Ceph正在执行迁移或同步对象和他们的副本         |
| incomplete   | 未完成状态。实际的副本数少于min_size。Ceph检测到PG正在丢失关于已经写操作的信息，或者没有任何健康的副本。如果遇到这种状态，尝试启动失败的OSD，这些OSD中可能包含需要的信息或者临时调整副本min_size的值到允许恢复。 |
| stale        | 未刷新状态。PG状态没有被任何OSD更新，这说明所有存储这个PG的OSD可能down |
| backfilling  | 正在后台填充状态。 当一个新的OSD加入集群后，Ceph通过移动一些其他OSD上的PG到新的OSD来达到新的平衡；这个过程完成后，这个OSD可以处理客户端的IO请求。 |
| remapped     | 重新映射状态。PG活动集任何的一个改变，数据发生从老活动集到新活动集的迁移。在迁移期间还是用老的活动集中的主OSD处理客户端请求，一旦迁移完成新活动集中的主OSD开始处理。 |
| undersized   | PG当前Acting Set小于存储池副本数                             |

# 常见故障模拟

## CLUSTERMON剩余磁盘空间偏低

```python
 if key == 'MON_DISK_CRIT' or key == 'MON_DISK_LOW':
    if cos_component in raw_desc:
        return '{0}! 部分{{{{cos_component}}}}的剩余磁盘空间偏低|@|{{"cos_component":"CLUSTERMON"}}'.format(key);
```

模拟方法：在mon所在节点dd一个大文件。

![image-20201226145535732](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226145535732.png)

![image-20201226140107419](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226140107419.png)

monitor磁盘占满后，monitor会自杀挂掉，因此稍等就会出现“集群监控不在运行”的错误。

![image-20201226140218617](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226140218617.png)

![image-20201226140231269](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226140231269.png)

**触发规则**

当可用比例小于mon_data_avail_crit 时，触发`MON_DISK_CRIT`；

当剩余空间比例小于`mon_data_avail_warn`时，触发`MON_DISK_LOW`：

剩余空间则是调用了各自的文件系统来获得（FileStore, BlueStore, MemStore等）。

```c++
bool HealthMonitor::check_member_health() {
  ...
  // MON_DISK_{LOW,CRIT,BIG}
  health_check_map_t next;
  if (stats.fs_stats.avail_percent <= g_conf()->mon_data_avail_crit) {
    stringstream ss, ss2;
    ss << "mon%plurals% %names% %isorare% very low on available space";
    auto& d = next.add("MON_DISK_CRIT", HEALTH_ERR, ss.str());
    ss2 << "mon." << mon->name << " has " << stats.fs_stats.avail_percent	<< "% avail";
    d.detail.push_back(ss2.str());
  } else if (stats.fs_stats.avail_percent <= g_conf()->mon_data_avail_warn) {
    stringstream ss, ss2;
    ss << "mon%plurals% %names% %isorare% low on available space";
    auto& d = next.add("MON_DISK_LOW", HEALTH_WARN, ss.str());
    ss2 << "mon." << mon->name << " has " << stats.fs_stats.avail_percent	<< "% avail";
    d.detail.push_back(ss2.str());
  }
  ...
}
```

可以看到该两个值的默认值分别为 5和30：

```c++
//options.cc
Option("mon_data_avail_crit", Option::TYPE_INT, Option::LEVEL_ADVANCED)
    .set_default(5)
    .add_service("mon")
    .set_description("issue MON_DISK_CRIT health error when mon available space below this percentage"),

Option("mon_data_avail_warn", Option::TYPE_INT, Option::LEVEL_ADVANCED)
  .set_default(30)
  .add_service("mon")
  .set_description("issue MON_DISK_LOW health warning when mon available space below this percentage"),
```

## CLUSTERMON所在的服务器之间存在较大时钟漂移

```python
  if key == 'MON_CLOCK_SKEW':
    return '{0}! {{{{cos_component}}}}所在的服务器之间存在较大时钟漂移|@|{{"cos_component":"CLUSTERMON"}}'.format(key)
```

## 部分CLUSTERMON占用了过多磁盘空间

```python
  if key == 'MON_DISK_BIG':
      if cos_component in raw_desc:
          return '{0}! 部分{{{{cos_component}}}}占用了过多磁盘空间|@|{{"cos_component":"CLUSTERMON"}}'.format(key)
```

当使用容量大于`mon_data_size_warn`时则会触发该告警：

```c++
bool HealthMonitor::check_member_health() {
  ...
    //注意此处判断的是store_stats里的使用容量，而不是fs_stats的使用容量。
   if (stats.store_stats.bytes_total >= g_conf()->mon_data_size_warn) {
    stringstream ss, ss2;
    ss << "mon%plurals% %names% %isorare% using a lot of disk space";
    auto& d = next.add("MON_DISK_BIG", HEALTH_WARN, ss.str());
    ss2 << "mon." << mon->name << " is "
	<< byte_u_t(stats.store_stats.bytes_total)
	<< " >= mon_data_size_warn ("
	<< byte_u_t(g_conf()->mon_data_size_warn) << ")";
    d.detail.push_back(ss2.str());
  }
  ...
}
```

可以看到`mon_data_size_warn`默认设置为15GB：

```c++
//options.cc
Option("mon_data_size_warn", Option::TYPE_SIZE, Option::LEVEL_ADVANCED)
  .set_default(15_G)
  .add_service("mon")
  .set_description("issue MON_DISK_BIG health warning when mon database is above this size"),
```

那么可以通过在ceph.conf中设置该值来，然后写入大于该值来进行构造：

```shell
#查看当前设置：
[root@localhost ceph]# ceph --admin-daemon ceph.CLUSTERMON_A-mon.192.168.56.201-CLUSTERMON_A.asok  config get mon_data_size_warn
{
    "mon_data_size_warn": "16106127360"
}

#设置一个较小的值：
[root@localhost ceph]# ceph --admin-daemon ceph.CLUSTERMON_A-mon.192.168.56.201-CLUSTERMON_A.asok  config set mon_data_size_warn 10M
{
    "success": "mon_data_size_warn = '10000000' (not observed, change may require restart) "
}
[root@localhost ceph]# ceph --admin-daemon ceph.CLUSTERMON_A-mon.192.168.56.201-CLUSTERMON_A.asok  config get mon_data_size_warn
{
    "mon_data_size_warn": "10000000"
}
```

可以看到：

![image-20201226174331549](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226174331549.png)

当将该值设置回去，则告警一会便会消失。

monitor主要是用于保存了monitor map, osd map, mds map。可以看到

```shell
# Erwa, add_mon.sh.j2
ceph-mon -i {{cos_mon_id}} \
            -c {{cos_mon_conf}} \
            --mon_data {{cos_mon_dir}} &
```

此处的cos_mon_dir从params.py中获得:

```python
#Erwa, params.py
cos_mon_dir = cos_dir + "/mon/mon.%s" % cos_component
```

在环境中，可以看到：

```shell
[root@TENCENT64 /data/cos/mon]# ls
mgr.CLUSTERMON_C  mon.CLUSTERMON_C  restapi.CLUSTERMON_C
[root@TENCENT64 /data/cos/mon/mon.CLUSTERMON_C]# ls
keyring  kv_backend  store.db
[root@TENCENT64 /data/cos/mon/mon.CLUSTERMON_C]# cat kv_backend
rocksdb
[root@TENCENT64 /data/cos/mon/mon.CLUSTERMON_C]# ls -lhrt store.db/
total 73M
-rw-r--r-- 1 root root     0 Dec 25 15:49 LOCK
-rw-r--r-- 1 root root    37 Dec 25 15:49 IDENTITY
-rw-r--r-- 1 root root  4.1K Dec 29 19:12 OPTIONS-006033
-rw-r--r-- 1 root root  4.1K Dec 29 19:43 OPTIONS-006054
-rw-r--r-- 1 root root    16 Dec 29 19:43 CURRENT
-rw-r--r-- 1 root root   34M Dec 29 20:21 006096.sst
-rw-r--r-- 1 root root  2.5K Dec 29 20:21 MANIFEST-006051
-rw-r--r-- 1 root root 1008K Dec 29 20:21 006094.log
```

设置保存mon map的后端存储引擎：

```c++
//ceph, MonitorDBStore.h
void _open(string kv_type) {
    string::const_reverse_iterator rit;
    int pos = 0;
    for (rit = path.rbegin(); rit != path.rend(); ++rit, ++pos) {
      if (*rit != '/')
	break;
    }
    ostringstream os;
    os << path.substr(0, path.size() - pos) << "/store.db";
    string full_path = os.str();
		//创建mon db
    KeyValueDB *db_ptr = KeyValueDB::create(g_ceph_context,
					    kv_type,
					    full_path);
    if (!db_ptr) {
      derr << __func__ << " error initializing "
	   << kv_type << " db back storage in "
	   << full_path << dendl;
      assert(0 == "MonitorDBStore: error initializing keyvaluedb back storage");
    }
    db.reset(db_ptr);
  ...
  }
}
```

容易看到上面的存储引擎为rocksdb，就以此为例说明：

rocksdb数据结构：

```c++
//ceph, RocksDBStore.h
/**
 * Uses RocksDB to implement the KeyValueDB interface
 */
class RocksDBStore : public KeyValueDB {
  CephContext *cct;
  PerfCounters *logger;
  string path; //上面传入的...../store.db路径
  void *priv;
#ifdef HAVE_TITANDB
  rocksdb::titandb::TitanDB *db;
#else
  rocksdb::DB *db;
#endif
  rocksdb::Env *env;
  std::shared_ptr<rocksdb::Statistics> dbstats;
  rocksdb::BlockBasedTableOptions bbt_opts;
  string options_str;

  uint64_t cache_size = 0;
  bool set_cache_flag = false;

  int do_open(ostream &out, bool create_if_missing);

  // manage async compactions
  Mutex compact_queue_lock;
  Cond compact_queue_cond;
  list< pair<string,string> > compact_queue;
  bool compact_queue_stop;
  class CompactThread : public Thread {
    RocksDBStore *db;
  public:
    explicit CompactThread(RocksDBStore *d) : db(d) {}
    void *entry() override {
      db->compact_thread_entry();
      return NULL;
    }
    friend class RocksDBStore;
  } compact_thread;

  void compact_thread_entry();

  void compact_range(const string& start, const string& end);
  void compact_range_async(const string& start, const string& end);

public:
  /// compact the underlying rocksdb store
  bool compact_on_mount;
  bool disableWAL;
  bool enable_rmrange;
  void compact() override;
  int64_t high_pri_watermark;
```

查看获取容量的位置，实际上是获取了这个目录下的文件大小之和：

```c++
uint64_t get_estimated_size(map<string,uint64_t> &extra) override {
    DIR *store_dir = opendir(path.c_str());
    if (!store_dir) {
      lderr(cct) << __func__ << " something happened opening the store: "
                 << cpp_strerror(errno) << dendl;
      return 0;
    }

    uint64_t total_size = 0;
    uint64_t sst_size = 0;
    uint64_t log_size = 0;
    uint64_t misc_size = 0;

    struct dirent *entry = NULL;
    while ((entry = readdir(store_dir)) != NULL) {
      string n(entry->d_name);

      if (n == "." || n == "..")
        continue;

      string fpath = path + '/' + n;
      struct stat s;
      int err = stat(fpath.c_str(), &s);
      if (err < 0)
	err = -errno;
      // we may race against rocksdb while reading files; this should only
      // happen when those files are being updated, data is being shuffled
      // and files get removed, in which case there's not much of a problem
      // as we'll get to them next time around.
      if (err == -ENOENT) {
	continue;
      }
      if (err < 0) {
        lderr(cct) << __func__ << " error obtaining stats for " << fpath
                   << ": " << cpp_strerror(err) << dendl;
        goto err;
      }

      size_t pos = n.find_last_of('.');
      if (pos == string::npos) {
        misc_size += s.st_size;
        continue;
      }

      string ext = n.substr(pos+1);
      if (ext == "sst") {
        sst_size += s.st_size;
      } else if (ext == "log") {
        log_size += s.st_size;
      } else {
        misc_size += s.st_size;
      }
    }

    total_size = sst_size + log_size + misc_size;

    extra["sst"] = sst_size;
    extra["log"] = log_size;
    extra["misc"] = misc_size;
    extra["total"] = total_size;

err:
    closedir(store_dir);
    return total_size;
  }
```

在Mon检查的时候，将其获得，与mon_data_size_warn做比较：

```c++
bool HealthMonitor::check_member_health() {
  ...
  map<string,uint64_t> extra;
  uint64_t store_size = mon->store->get_estimated_size(extra);
  assert(store_size > 0);
  stats.store_stats.bytes_total = store_size;
  stats.store_stats.bytes_sst = extra["sst"];
  stats.store_stats.bytes_log = extra["log"];
  stats.store_stats.bytes_misc = extra["misc"];
  stats.last_update = ceph_clock_now();
  ...
    if (stats.store_stats.bytes_total >= g_conf->mon_data_size_warn) {
      stringstream ss, ss2;
      ss << "mon%plurals% %names% %isorare% using a lot of disk space";
      auto& d = next.add("MON_DISK_BIG", HEALTH_WARN, ss.str());
      ss2 << "mon." << mon->name << " is "
        << byte_u_t(stats.store_stats.bytes_total)
        << " >= mon_data_size_warn ("
        << byte_u_t(g_conf->mon_data_size_warn) << ")";
      d.detail.push_back(ss2.str());
    }
```

## N个磁盘已满，将不允许数据写入

```python
  if key == 'OSD_FULL':
    m = re.match(r'(?P<num>[0-9]+) full osd\(s\)', raw_desc)
    if m:
      return '{1}! {{{{num}}}}个磁盘已满，将不允许数据写入|@|{{"num":"{0}"}}'.format(m.group('num'), key)
```

![image-20201226140714155](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226140714155.png)

模拟方法1：在datastor目录中，使用dd写满磁盘：

![image-20201226140655791](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226140655791.png)

模拟方法2：` injectfull full`

<img src=".Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226140918383.png" alt="image-20201226140918383" style="zoom:50%;" />

代码：当

```c++
//OSDMap.cc
void OSDMap::check_health(CephContext *cct, health_check_map_t *checks) const {
  ...
    set<int> full, backfillfull, nearfull;
    get_full_osd_counts(&full, &backfillfull, &nearfull);
    if (full.size()) {
      ostringstream ss;
      ss << full.size() << " full osd(s)";
      auto& d = checks->add("OSD_FULL", HEALTH_ERR, ss.str());
      for (auto& i: full) {
	ostringstream ss;
	ss << "osd." << i << " is full";
	d.detail.push_back(ss.str());
      }
}
```

设置：

```c++
//OSD.cc
//发送FULL状态
void OSD::send_full_update() {
  if (!service.need_fullness_update())
    return;
  unsigned state = 0;
  if (service.is_full()) {
    state = CEPH_OSD_FULL;
  } else if (service.is_backfillfull()) {
    state = CEPH_OSD_BACKFILLFULL;
  } else if (service.is_nearfull()) {
    state = CEPH_OSD_NEARFULL;
  }
  set<string> s;
  OSDMap::calc_state_set(state, s);
  dout(10) << __func__ << " want state " << s << dendl;
  monc->send_mon_message(new MOSDFull(get_osdmap_epoch(), state));
}

//若状态需要更新，则发送：
bool OSDService::need_fullness_update() {
  OSDMapRef osdmap = get_osdmap();
  s_names cur = NONE;
  if (osdmap->exists(whoami)) {
    if (osdmap->get_state(whoami) & CEPH_OSD_FULL) {
      cur = FULL;
    } else if (osdmap->get_state(whoami) & CEPH_OSD_BACKFILLFULL) {
      cur = BACKFILLFULL;
    } else if (osdmap->get_state(whoami) & CEPH_OSD_NEARFULL) {
      cur = NEARFULL;
    }
  }
  s_names want = NONE;
  if (is_full())
    want = FULL;
  else if (is_backfillfull())
    want = BACKFILLFULL;
  else if (is_nearfull())
    want = NEARFULL;
  return want != cur;
}

//最终判断full的位置
OSDService::s_names OSDService::recalc_full_state(float ratio, float pratio, string &inject)
{
  // The OSDMap ratios take precendence.  So if the failsafe is .95 and
  // the admin sets the cluster full to .96, the failsafe moves up to .96
  // too.  (Not that having failsafe == full is ideal, but it's better than
  // dropping writes before the clusters appears full.)
  OSDMapRef osdmap = get_osdmap();
  if (!osdmap || osdmap->get_epoch() == 0) {
    return NONE;
  }
  float nearfull_ratio = osdmap->get_nearfull_ratio();
  float backfillfull_ratio = std::max(osdmap->get_backfillfull_ratio(), nearfull_ratio);
  float full_ratio = std::max(osdmap->get_full_ratio(), backfillfull_ratio);
  float failsafe_ratio = std::max(get_failsafe_full_ratio(), full_ratio);

  if (osdmap->require_osd_release < CEPH_RELEASE_LUMINOUS) {
    // use the failsafe for nearfull and full; the mon isn't using the
    // flags anyway because we're mid-upgrade.
    full_ratio = failsafe_ratio;
    backfillfull_ratio = failsafe_ratio;
    nearfull_ratio = failsafe_ratio;
  } else if (full_ratio <= 0 ||
	     backfillfull_ratio <= 0 ||
	     nearfull_ratio <= 0) {
    derr << __func__ << " full_ratio, backfillfull_ratio or nearfull_ratio is <= 0" << dendl;
    // use failsafe flag.  ick.  the monitor did something wrong or the user
    // did something stupid.
    full_ratio = failsafe_ratio;
    backfillfull_ratio = failsafe_ratio;
    nearfull_ratio = failsafe_ratio;
  }

  if (injectfull_state > NONE && injectfull) {
    inject = "(Injected)";
    return injectfull_state;
  } else if (pratio > failsafe_ratio) {
    return FAILSAFE;
  } else if (ratio > full_ratio) {
    return FULL;
  } else if (ratio > backfillfull_ratio) {
    return BACKFILLFULL;
  } else if (pratio > nearfull_ratio) {
    return NEARFULL;
  }
   return NONE;
}
```





## N个存储池已满，将会禁止数据写入

> 见上一小节。

集群中某个OSD满了，就认为整个存储池满了。因为数据分布是按权重分布的，理论上占用磁盘空间比例应该是一致的，且满了以后也应该是同时满的。

```python
if key == 'POOL_FULL':
    m = re.match(r'(?P<num>[0-9]+) pools full', raw_desc)
    if m:
      return '{1}! {{{{num}}}}个存储池已满，将会禁止数据写入|@|{{"num":"{0}"}}'.format(m.group('num'), key)
```



## N个磁盘剩余空间不足，将不允许数据迁入

```python
if key == 'OSD_BACKFILLFULL':
    m = re.match(r'(?P<num>[0-9]+) backfillfull osd\(s\)', raw_desc)
    if m:
      return '{1}! {{{{num}}}}个磁盘剩余空间不足，将不允许数据迁入|@|{{"num":"{0}"}}'.format(m.group('num'), key)
```

注入方法：

```shell
ceph --admin-daemon ceph.DATASTOR_A-osd.1.asok injectfull backfillfull
```

另外方法：

在backfill的过程中写满数据。

![image-20201226143025135](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226143025135.png)

![image-20201226143042745](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226143042745.png)

<img src=".Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226143005371.png" alt="image-20201226143005371" style="zoom: 50%;" />

代码：

```c++
//OSDMap.cc
void OSDMap::check_health(CephContext *cct, health_check_map_t *checks) const {
  ...
  set<int> full, backfillfull, nearfull;
  get_full_osd_counts(&full, &backfillfull, &nearfull);
  if (backfillfull.size()) {
      ostringstream ss;
      ss << backfillfull.size() << " backfillfull osd(s)";
      auto& d = checks->add("OSD_BACKFILLFULL", HEALTH_WARN, ss.str());
      for (auto& i: backfillfull) {
	ostringstream ss;
	ss << "osd." << i << " is backfill full";
	d.detail.push_back(ss.str());
      }
    }
  ...
}
```



## N个磁盘剩余空间过低，请及时处理

```python
 if key == 'OSD_NEARFULL':
    m = re.match(r'(?P<num>[0-9]+) nearfull osd\(s\)', raw_desc)
    if m:
      return '{1}! {{{{num}}}}个磁盘剩余空间过低，请及时处理|@|{{"num":"{0}"}}'.format(m.group('num'), key)
```

![image-20201226142558232](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226142558232.png)

![image-20201226142628339](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226142628339.png)

注入方法：

```shell
# 找到osd对应的socket
cd /var/run/ceph/
# 注入nearfull故障：
[root@localhost ceph]# ceph --admin-daemon ceph.DATASTOR_A-osd.1.asok injectfull nearfull
```

<img src=".Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201226142825495.png" alt="image-20201226142825495" style="zoom:50%;" />



```c++
//OSDMap.cc
void OSDMap::check_health(CephContext *cct, health_check_map_t *checks) const {
  ...
  get_full_osd_counts(&full, &backfillfull, &nearfull);
  if (nearfull.size()) {
      ostringstream ss;
      ss << nearfull.size() << " nearfull osd(s)";
      auto& d = checks->add("OSD_NEARFULL", HEALTH_WARN, ss.str());
      for (auto& i: nearfull) {
	ostringstream ss;
	ss << "osd." << i << " is near full";
	d.detail.push_back(ss.str());
      }
    }
  ...
}
```



## N个存储池剩余容量过低，请及时处理

> 见上一小节。

```python
if key == 'POOL_NEAR_FULL':
    m = re.match(r'(?P<num>[0-9]+) pools nearfull', raw_desc)
    if m:
      return '{1}! {{{{num}}}}个存储池剩余容量过低，请及时处理|@|{{"num":"{0}"}}'.format(m.group('num'), key)
```



## 副本数不足，需要恢复

```python
if key == 'PG_AVAILABILITY':
    return '{0}! 部分数据不可用|@|{{}}'.format(key)

if key == 'PG_DEGRADED':
    m = re.match(r'.*/.* objects degraded \((?P<percent>[.0-9]+%)\)', raw_desc)
    if m:
      return '{1}! {{{{percent}}}}的数据副本数不足，需要恢复|@|{{"percent":"{0}"}}'.format(m.group('percent'),key)
```

![image-20201225152740364](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201225152740364.png)

![image-20201225152758583](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201225152758583.png)

![image-20201225152820980](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201225152820980.png)

构造方法：让一个osd挂掉即可：

```shell
ceph -c $CONF osd set noup
ceph-c $CONF osd down osd.xxx
```

恢复方法：将noup标记unset掉：

```shell
ceph -c $CONF osd unset noup
```

## 由于磁盘空间不足导致部分数据副本数不足

> ==在ceph N版中去掉了这个告警，为什么？以下基于L版。==


```python
if key == 'PG_DEGRADED_FULL':
    return '{0}! 由于磁盘空间不足导致部分数据副本数不足|@|{{}}'.format(key)
```

PG_DEGRADED_FULL

由于集群中的可用空间不足，某些数据的数据冗余可能已降低或面临风险。具体而言，一个或多个 PG 设置了 *backfill_toofull* 或 *recovery_tooful* 标志，这意味着集群无法迁移或恢复数据，原因是一个或多个 OSD 高于 *backfillfull* 阈值。

> PG_DEGRADED_FULL
> ________________
>
> Data redundancy may be reduced or at risk for some data due to a lack
> of free space in the cluster.  Specifically, one or more PGs has the
> *backfill_toofull* or *recovery_toofull* flag set, meaning that the
> cluster is unable to migrate or recover data because one or more OSDs
> is above the *backfillfull* threshold.
>
> See the discussion for *OSD_BACKFILLFULL* or *OSD_FULL* above for
> steps to resolve this condition.

```c++
//PGMap.cc 
case DEGRADED_FULL:
        health_code = "PG_DEGRADED_FULL";
        summary = "Degraded data redundancy (low space): ";
        sev = HEALTH_ERR;
```

**构造方法**

让其在backfill的时候，目标osd超过backfillfull阈值：

触发backfill的时机：

recovery：依据PG日志来推测不一致的对象加以修复。当某个OSD长时间失效后重新加入集群

> mon_osd_backfillfull_ratio设小一点，然后让一个osd out，这样会触发rebalance，这样会backfill； 

标记osd为out时将触发数据重新平衡的操作。

默认值如下：

```c++
//options.cc
Option("mon_osd_full_ratio", Option::TYPE_FLOAT, Option::LEVEL_ADVANCED)
    .set_default(.95)
    .set_description(""),

    Option("mon_osd_backfillfull_ratio", Option::TYPE_FLOAT, Option::LEVEL_ADVANCED)
    .set_default(.90)
    .set_description(""),

    Option("mon_osd_nearfull_ratio", Option::TYPE_FLOAT, Option::LEVEL_ADVANCED)
    .set_default(.85)
    .set_description(""),
```

可以通过命令行设置：

```c++
COMMAND("osd set-full-ratio " \
	"name=ratio,type=CephFloat,range=0.0|1.0", \
	"set usage ratio at which OSDs are marked full",
	"osd", "rw", "cli,rest")
COMMAND("osd set-backfillfull-ratio " \
	"name=ratio,type=CephFloat,range=0.0|1.0", \
	"set usage ratio at which OSDs are marked too full to backfill",
	"osd", "rw", "cli,rest")
COMMAND("osd set-nearfull-ratio " \
	"name=ratio,type=CephFloat,range=0.0|1.0", \
	"set usage ratio at which OSDs are marked near-full",
	"osd", "rw", "cli,rest")
```

查看这几个参数的设置：

```shell
ceph -c $CONF pg dump
```

这里在调整的时候，是有一定逻辑限制的：即nr >= br >= fr。这样才是符合逻辑的。

```c++
//OSDMap.cc
void OSDMap::check_health(health_check_map_t *checks) const
{
  ...
    // An osd could configure failsafe ratio, to something different
    // but for now assume it is the same here.
    float fsr = g_conf->osd_failsafe_full_ratio;
    if (fsr > 1.0) fsr /= 100;
    float fr = get_full_ratio();//默认.95
    float br = get_backfillfull_ratio();//默认.90
    float nr = get_nearfull_ratio();//默认.85

    list<string> detail;
    // These checks correspond to how OSDService::check_full_status() in an OSD
    // handles the improper setting of these values.
    if (br < nr) {
      ostringstream ss;
      ss << "backfillfull_ratio (" << br
	 << ") < nearfull_ratio (" << nr << "), increased";
      detail.push_back(ss.str());
      br = nr;
    }
    if (fr < br) {
      ostringstream ss;
      ss << "full_ratio (" << fr << ") < backfillfull_ratio (" << br
	 << "), increased";
      detail.push_back(ss.str());
      fr = br;
    }
    if (fsr < fr) {
      ostringstream ss;
      ss << "osd_failsafe_full_ratio (" << fsr << ") < full_ratio (" << fr
	 << "), increased";
      detail.push_back(ss.str());
    }
    if (!detail.empty()) {
      auto& d = checks->add("OSD_OUT_OF_ORDER_FULL", HEALTH_ERR,
			 "full ratio(s) out of order");
      d.detail.swap(detail);
    }
  }

...
}
```

**测试**

```shell
ceph -c $CONF osd set-nearfull-ratio 0.01
ceph -c $CONF osd set-backfillfull-ratio 0.012
ceph -c $CONF -s
ceph -c $CONF osd set noup
ceph -c $CONF osd tree
ceph -c $CONF osd down osd.1
ceph -c $CONF -s
ceph -c $CONF osd out osd.1
watch ceph -c $CONF -s

#恢复原状
ceph -c $CONF osd set-nearfull-ratio 0.85
ceph -c $CONF osd set-backfillfull-ratio 0.9
ceph -c $CONF osd unset noup
ceph -c $CONF osd in osd.1
```



<img src=".Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20210105165940074.png" alt="image-20210105165940074" style="zoom:50%;" />

<img src=".Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20210105170035974.png" alt="image-20210105170035974" style="zoom:50%;" />

![image-20210105170001866](Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20210105170001866.png)





## 部分数据出现副本不一致

```python
if key == 'PG_DAMAGED':
    return '{0}! 部分数据出现副本不一致|@|{{}}'.format(key)
```

![image-20201231142818028](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/image-20201231142818028.png)

> PG_DAMAGED
>
> __________
>
> Data scrubbing has discovered some problems with data consistency in
> the cluster.  Specifically, one or more PGs has the *inconsistent* or
> *snaptrim_error* flag is set, indicating an earlier scrub operation
> found a problem, or that the *repair* flag is set, meaning a repair
> for such an inconsistency is currently in progress.
>
> See :doc:`pg-repair` for more information.
>
> OSD_SCRUB_ERRORS
>
> ________________
>
> Recent OSD scrubs have uncovered inconsistencies. This error is generally
> paired with *PG_DAMAGED* (see above).
>
> See :doc:`pg-repair` for more information.

查看ceph代码可以发现`PG_DAMAGED`的出现场景：

```c++
//PGMap.cc
// Map of PG state to how to respond to it
std::map<unsigned, PgStateResponse> state_to_response = {
  // Immediate reports
  { PG_STATE_INCONSISTENT,     {DAMAGED,     {}} },
  { PG_STATE_SNAPTRIM_ERROR,   {DAMAGED,     {}} },
  { PG_STATE_RECOVERY_UNFOUND, {DAMAGED,     {}} },
  { PG_STATE_BACKFILL_UNFOUND, {DAMAGED,     {}} },
  ...
}

... 
case DAMAGED:
        health_code = "PG_DAMAGED";
        summary = "Possible data damage: ";
        sev = HEALTH_ERR;
        break;
```

构造方法：

```shell
1.上传一个对象
2.找到该对象在osd的保存位置，记录pgid，进入到osd的目录，然后删除它
3.对该pgid手动启用scrub:
4.对该pgid进行repair:

[root@localhost ~]# touch object1
[root@localhost ~]# echo "object1~~~" > object1 
[root@localhost ~]# rados -c $CONF -p 00000000-default.rgw.buckets.data put object1 object1
[root@localhost ~]# rados -c $CONF -p 00000000-default.rgw.buckets.data ls 
object1
[root@localhost ~]# ceph -c $CONF osd map 00000000-default.rgw.buckets.data object1
osdmap e250 pool '00000000-default.rgw.buckets.data' (7) object 'object1' -> pg 7.bac5debc (7.bc) -> up ([5,3,1], p5) acting ([5,3,1], p5)
[root@localhost ~]# ceph -c $CONF osd tree
ID  CLASS WEIGHT  TYPE NAME                                        STATUS REWEIGHT PRI-AFF 
 -6       0.02930 root 00000000-default                                                    
 -5       0.02930     datacenter 00000000-default-datacenter.dc1                           
 -4       0.02930         rack 00000000-default-rack.r1                                    
-13       0.00977             host 00000000-default-192.168.56.200                         
  2   hdd 0.00488                 osd.2                                up  1.00000 1.00000 
  5   hdd 0.00488                 osd.5                                up  1.00000 1.00000 
 -3       0.00977             host 00000000-default-192.168.56.201                         
  0   hdd 0.00488                 osd.0                                up  1.00000 1.00000 
  3   hdd 0.00488                 osd.3                                up  1.00000 1.00000 
-11       0.00977             host 00000000-default-192.168.56.202                         
  1   hdd 0.00488                 osd.1                                up  1.00000 1.00000 
  4   hdd 0.00488                 osd.4                                up  1.00000 1.00000 
 -1             0 root default                                                             
[root@localhost ~]# ps aux | grep osd
root        5682  0.1  6.8 1203644 69260 ?       Ssl  Dec30   0:29 ceph-osd -i 3 -c /data/cos/ceph.DATASTOR_B.conf --osd_data=/data/cos/osd/osd.DATASTOR_B --osd_journal=/data/cos/osd/osd.DATASTOR_B/journal
root       88137  0.3 14.0 1169916 142708 ?      Ssl  01:04   0:06 ceph-osd -i 0 -c /data/cos/ceph.DATASTOR_A.conf --osd_data=/data/cos/osd/osd.DATASTOR_A --osd_journal=/data/cos/osd/osd.DATASTOR_A/journal
root      102340  0.0  0.0 112712   960 pts/0    R+   01:39   0:00 grep --color=auto osd
[root@localhost ~]# cd /data/cos/osd/osd.DATASTOR_B/current/7.bc_head/
[root@localhost 7.bc_head]# ls
a483845e-0142-4016-99a8-4a545a1d59aa.15853.2\u\umultipart\uAWS S3开发者文档.pdf.2~t5yLy4dWchkBiLfCrmMjjIJHfgvZonE.1__head_5D22F0BC__7
__head_000000BC__7
object1__head_BAC5DEBC__7
[root@localhost 7.bc_head]# rm object1__head_BAC5DEBC__7 
rm: remove regular file ‘object1__head_BAC5DEBC__7’? y
[root@localhost 7.bc_head]# ceph -c $CONF pg scrub 7.bc
instructing pg 7.bc on osd.5 to scrub
[root@localhost 7.bc_head]# ceph -c $CONF -s
  cluster:
    id:     e25221bc-4fb9-4a67-a745-4a5c5737d119
    health: HEALTH_ERR
            1 scrub errors
            Possible data damage: 1 pg inconsistent
 
  services:
    mon: 3 daemons, quorum 192.168.56.200-CLUSTERMON_A,192.168.56.201-CLUSTERMON_B,192.168.56.202-CLUSTERMON_C (age 4h)
    mgr: CLUSTERMGR-192.168.56.201(active, since 4h), standbys: CLUSTERMGR-192.168.56.200, CLUSTERMGR-192.168.56.202
    osd: 6 osds: 6 up (since 36m), 6 in (since 38m)
    rgw: 1 daemon active (DATAACCESS)
 
  task status:
 
  data:
    pools:   7 pools, 596 pgs
    objects: 1.47k objects, 11 MiB
    usage:   737 MiB used, 23 GiB / 24 GiB avail
    pgs:     595 active+clean
             1   active+clean+inconsistent
#修复PG_DAMAGED
[root@localhost 7.bc_head]# ceph -c $CONF pg repair 7.bc
instructing pg 7.bc on osd.5 to repair
[root@localhost 7.bc_head]# ceph -c $CONF -s 
  cluster:
    id:     e25221bc-4fb9-4a67-a745-4a5c5737d119
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum 192.168.56.200-CLUSTERMON_A,192.168.56.201-CLUSTERMON_B,192.168.56.202-CLUSTERMON_C (age 4h)
    mgr: CLUSTERMGR-192.168.56.201(active, since 4h), standbys: CLUSTERMGR-192.168.56.200, CLUSTERMGR-192.168.56.202
    osd: 6 osds: 6 up (since 38m), 6 in (since 41m)
    rgw: 1 daemon active (DATAACCESS)
 
  task status:
 
  data:
    pools:   7 pools, 596 pgs
    objects: 1.47k objects, 11 MiB
    usage:   737 MiB used, 23 GiB / 24 GiB avail
    pgs:     596 active+clean
 
  io:
    recovery: 0 B/s, 0 objects/s
 
```

## 缓存的剩余容量偏低（？）

```python
if key == 'CACHE_POOL_NEAR_FULL':
    return '{0}! 缓存的剩余容量偏低|@|{{}}'.format(key)
```

## 数据需要重分布

```python
if key == 'OBJECT_MISPLACED':
    m = re.match(r'.*/.* objects misplaced \((?P<percent>[.0-9]+%)\)', raw_desc)
    if m:
      return '{1}! {{{{percent}}}}的数据需要重分布|@|{{"percent":"{0}"}}'.format(m.group('percent'), key)
```

> One or more objects in the cluster is not stored on the node the cluster would like it to be stored on. This is an indication that data migration due to some recent cluster change has not yet completed.
>
> Misplaced data is not a dangerous condition in and of itself; data consistency is never at risk, and old copies of objects are never removed until the desired number of new copies (in the desired locations) are present.

直接让一个osd标记为out即可：

```shell
ceph -c $CONF osd out osd.0
```

## 数据可能丢失

```python
if key == 'OBJECT_UNFOUND':
   m = re.match(r'.*/.* objects unfound \((?P<percent>[.0-9]+%)\)', raw_desc)
   if m:
   return '{1}! {{{{percent}}}}的数据可能丢失|@|{{"percent":"{0}"}}'.format(m.group('percent'), key)
```

![企业微信截图_dcaa5ecc-23cb-4841-b2ee-71a8b0df2fdf](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_dcaa5ecc-23cb-4841-b2ee-71a8b0df2fdf.png)

![企业微信截图_1ce1f0ce-24db-414c-8151-00ab8014953f](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_1ce1f0ce-24db-414c-8151-00ab8014953f.png)

void object_stat_sum_t::generate_test_instances(list<object_stat_sum_t*>

object_stat_collection_t::generate_test_instances

https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-pg/

**构造方法1：**

第三个OBJECT_UNFOUND：应该是找一个PG对应的几个osd，然后让一个osd down，再写一些数据到这个PG上，这时这个PG应该知道有数据更新，然后让那个osd up，会触发peering，在没有peering完成时让这个PG所有之前有新数据的osd都挂掉

**构造方法2（更好控制）:**

> 有一个方法可以试验一下，假设总共3个台机器OSD，pool的副本数为3
>1. 故意禁用其中一台机器
> 2. rados put写入一个对象
> 3. 到OSD的目录里把那个对象对应的文件删除
> 4. 把禁用的机器启动起来

```shell
#1. 禁用一个节点

#2. 在任何其余存活节点rados put一个对象
touch object3
echo "object3~~~~~~~~~" > object3 
rados -c $CONF -p 00000000-default.rgw.buckets.data put object3 object3
ceph -c $CONF osd map  00000000-default.rgw.buckets.data object3
#3. 进入所有存活节点，找到写入rados层的这个对象，删掉
cd /data/cos/osd/
find ./ -name *object3*
rm ./osd.DATASTOR_A/current/7.20_head/object3__head_F877AC20__7

#4. 启用刚刚禁用的机器
```

**恢复方法**

```shell
#查看有问题的PG,可以看到是pg 7.8bb
[root@TENCENT64 ~]# ceph -c $CONF health detail
HEALTH_WARN 48261/2210622 objects misplaced (2.183%); 1/736874 objects unfound (0.000%); Degraded data redundancy: 149035/2210622 objects degraded (6.742%), 371 pgs degraded
pg 7.8bb has 1 unfound objects
pg 7.da6 is active+recovery_wait+degraded+remapped, acting [19,4,0]
pg 7.dac is active+recovery_wait+degraded, acting [3,37,28]

#查看丢失的对象：
[root@TENCENT64 ~]# ceph -c $CONF pg 7.8bb list_missing
{
    "offset": {
        "oid": "",
        "key": "",
        "snapid": 0,
        "hash": 0,
        "max": 0,
        "pool": -9223372036854775808,
        "namespace": ""
    },
    "num_missing": 1,
    "num_unfound": 1,
    "objects": [
        {
            "oid": {
                "oid": "object_sher",
                "key": "",
                "snapid": -2,
                "hash": 2667178171,
                "max": 0,
                "pool": 7,
                "namespace": ""
            },
            "need": "3671'614",
            "have": "0'0",
            "flags": "none",
            "locations": []
        }
    ],
    "more": false
}

#可以标记这个对象为 revert或delete:
[root@TENCENT64 ~]# ceph -c $CONF pg 7.8bb mark_unfound_lost delete
pg has 1 objects unfound and apparently lost marking
```

## 部分请求处理缓慢，集群压力可能较大

```python
if key == 'REQUEST_SLOW':
    return '{0}! 部分请求处理缓慢，集群压力可能较大|@|{{}}'.format(key)
```

**出现场景**

如果一个OSD服务进程很慢地响应请求。它会产生一个请求耗时过久超过30秒的警告提示信息。

```
{date} {osd.num} [WRN] 1 slow requests, 1 included below; oldest blocked for > 30.005692 secs
{date} {osd.num}  [WRN] slow request 30.005692 seconds old, received at {date-time}: osd_op(client.4240.0:8 benchmark_data_ceph-1_39426_object7 [write 0~4194304] 0.69848840) v4 currently waiting for subops from [610]
```

**可能的原因和修复方法**

1. 硬盘故障（检查dmesg的输出信息）；替换为正常的硬盘
2. 内核文件系统bug（检查dmesg的输出信息确）；升级内核
3. 集群负载过高（检查系统负载、iostat等）；机器扩容，尝试降低系统负载
4. ceph-osd服务进程的的bug；升级ceph或重启OSD

Ceph集群慢请求解决思路 https://blog.csdn.net/lzw06061139/article/details/80239067

**场景模拟**

让PG还是UP的状态，让上层以为PG还可用，但是实际上让对应的osd挂掉（通过禁用节点），然后发到这个PG上面的请求就都不会返回，这样就造成了慢请求。

```shell
1. 在一个存储节点设置： ceph -c /data/cos/ceph.DATAACCESS.conf osd set nodown
2. 在运营端禁用一个节点
3. 用rados bench发一些数据：
rados -c /data/cos/ceph.CLUSTERMON_C.conf -p 00000000-default.rgw.buckets.data bench 10 write
```

![企业微信截图_a911e958-9bb8-4a51-88a0-6a874d3c79e8](.Ceph%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E6%A8%A1%E6%8B%9F%E5%8F%8A%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90.assets/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_a911e958-9bb8-4a51-88a0-6a874d3c79e8.png)



# 附录

## rados bench压测工具

ceph故障处理：http://www.tang-lei.com/2018/03/14/ceph-故障处理/

https://cloud.tencent.com/developer/article/1664615

常见osd故障处理：https://lihaijing.gitbooks.io/ceph-handbook/content/Troubleshooting/troubleshooting_osd.html

https://documentation.suse.com/zh-cn/ses/5.5/html/ses-all/ceph-monitor.html

## 用户态故障注入工具libfio

https://github.com/albertito/libfiu

```shell
LD_LIBRARY_PATH=/usr/local/lib 
export LD_LIBRARY_PATH
```



当然，上面2个例子你可能觉得这个工具作用不是很大，只能测测简单的程序，但实际上对于复杂程序也能测试的。比如测试ceph。

```
root@pubt1-ceph64:~# fiu-run -x -c "enable_random name=posix/io/*,probability=0.8" /etc/init.d/ceph start osd.3/etc/init.d/ceph: error reading input file: Input/output errorroot@pubt1-ceph64:~# fiu-run -x -c "enable_random name=posix/io/*,probability=0.8" /etc/init.d/ceph start osd.3/bin/sh: /etc/init.d/ceph: No such file or directory
```

libfiu具体支持哪些库我们也可以通过下面命令看到。类似于malloc() read() write() fork() mmap()以及一些 socket相关的都能模拟。

cat /root/libfiu-0.96/preload/posix/function_list

回到错误注入这个话题，提供一些常见的错误注入方法



Network Delay: Use tc like `tc qdisc add dev eth0 root netem delay 1000ms` to set network delay or recover.

Network Unavailable: Use iptable to block the specific port, like `iptables -A OUTPUT -p tcp --dport 3306 -j DROP `.

Network Bandwidth Limit: Use tc like `tc qdisc add dev eth0 root tbf rate 5800kbit latency 50ms burst 1540` to limit the bandwidth.

Disk Full: Use dd to create a really large file to fill up the disk, like `dd if=/dev/zero of=/$path/tst.img bs=1M count=20K `.

Disk Failure: Maybe use fiu-ctrl

Disk Slow: Use fio to write or read a lot making the disk under stress.

Memory Limit: Impl a program from [http://minuteware.net/simulating-high-memory-usage-in-linux/](https://links.jianshu.com/go?to=http%3A%2F%2Fminuteware.net%2Fsimulating-high-memory-usage-in-linux%2F) .

CPU Limit: Use cpulimit from [https://github.com/opsengine/cpulimit](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fopsengine%2Fcpulimit) .

POSIX: fiulib

作者：星夜兼程工作笔记
链接：https://www.jianshu.com/p/f4722f060f0f
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

## ceph官方的故障注入方法

在ceph代码的`qa/tasks/ceph_manager.py`中给出了各种故障注入的方法。

`qa/workuints/cephtool/test.sh`

## 注入OSD  FULL 相关故障

```shell
# Check injected full results
  $SUDO ceph --admin-daemon $(get_admin_socket osd.0) injectfull nearfull
  wait_for_health "OSD_NEARFULL"
  ceph health detail | grep "osd.0 is near full"
  $SUDO ceph --admin-daemon $(get_admin_socket osd.0) injectfull none
  wait_for_health_ok

  $SUDO ceph --admin-daemon $(get_admin_socket osd.1) injectfull backfillfull
  wait_for_health "OSD_BACKFILLFULL"
  ceph health detail | grep "osd.1 is backfill full"
  $SUDO ceph --admin-daemon $(get_admin_socket osd.1) injectfull none
  wait_for_health_ok

  $SUDO ceph --admin-daemon $(get_admin_socket osd.2) injectfull failsafe
  # failsafe and full are the same as far as the monitor is concerned
  wait_for_health "OSD_FULL"
  ceph health detail | grep "osd.2 is full"
  $SUDO ceph --admin-daemon $(get_admin_socket osd.2) injectfull none
  wait_for_health_ok

  $SUDO ceph --admin-daemon $(get_admin_socket osd.0) injectfull full
  wait_for_health "OSD_FULL"
  ceph health detail | grep "osd.0 is full"
  $SUDO ceph --admin-daemon $(get_admin_socket osd.0) injectfull none
  wait_for_health_ok

  ceph pg stat | grep 'pgs:'
  ceph pg 1.0 query
  ceph tell 1.0 query
  ceph quorum enter
  ceph quorum_status
  ceph report | grep osd_stats
  ceph status
  ceph -s
```



```python
def test_backfill_full(self):
        """
        Test backfills stopping when the replica fills up.

        First, use injectfull admin command to simulate a now full
        osd by setting it to 0 on all of the OSDs.

        Second, on a random subset, set
        osd_debug_skip_full_check_in_backfill_reservation to force
        the more complicated check in do_scan to be exercised.

        Then, verify that all backfillings stop.
        """
        self.log("injecting backfill full")
        for i in self.live_osds:
            self.ceph_manager.set_config(
                i,
                osd_debug_skip_full_check_in_backfill_reservation=
                random.choice(['false', 'true']))
            self.ceph_manager.osd_admin_socket(i, command=['injectfull', 'backfillfull'],
                                     check_status=True, timeout=30, stdout=DEVNULL)
        for i in range(30):
            status = self.ceph_manager.compile_pg_status()
            if 'backfilling' not in status.keys():
                break
            self.log(
                "waiting for {still_going} backfillings".format(
                    still_going=status.get('backfilling')))
            time.sleep(1)
        assert('backfilling' not in self.ceph_manager.compile_pg_status().keys())
        for i in self.live_osds:
            self.ceph_manager.set_config(
                i,
                osd_debug_skip_full_check_in_backfill_reservation='false')
            self.ceph_manager.osd_admin_socket(i, command=['injectfull', 'none'],
                                     check_status=True, timeout=30, stdout=DEVNULL)
```

实际生效的位置：

```c++
//OSD.cc
// Usage:
//   setomapval <pool-id> [namespace/]<obj-name> <key> <val>
//   rmomapkey <pool-id> [namespace/]<obj-name> <key>
//   setomapheader <pool-id> [namespace/]<obj-name> <header>
//   getomap <pool> [namespace/]<obj-name>
//   truncobj <pool-id> [namespace/]<obj-name> <newlen>
//   injectmdataerr [namespace/]<obj-name> [shardid]
//   injectdataerr [namespace/]<obj-name> [shardid]
//
//   set_recovery_delay [utime]
void TestOpsSocketHook::test_ops(OSDService *service, ObjectStore *store,
				 std::string_view command,
				 const cmdmap_t& cmdmap, ostream &ss)
{
  ...
  if (command == "injectfull") {
      int64_t count;
      string type;
      OSDService::s_names state;
      cmd_getval(service->cct, cmdmap, "type", type, string("full"));
      cmd_getval(service->cct, cmdmap, "count", count, (int64_t)-1);
      if (type == "none" || count == 0) {
        type = "none";
        count = 0;
      }
      state = service->get_full_state(type);
      if (state == OSDService::s_names::INVALID) {
        ss << "Invalid type use (none, nearfull, backfillfull, full, failsafe)";
        return;
      }
      service->set_injectfull(state, count);
      return;
    }
    ss << "Internal error - command=" << command;
  }
	...
}
```

在实际的full 状态计算的位置，第一步判断了是否是injectfull状态，若是则表示为注入测试，直接按照给定的值返回，否则将按照真实的计算值返回相应真实结果：

```c++
//OSD.cc
OSDService::s_names OSDService::recalc_full_state(float ratio, float pratio, string &inject) {
  if (injectfull_state > NONE && injectfull) {
      inject = "(Injected)";
      return injectfull_state;
    } else if (pratio > failsafe_ratio) {
      return FAILSAFE;
    } else if (ratio > full_ratio) {
      return FULL;
    } else if (ratio > backfillfull_ratio) {
      return BACKFILLFULL;
    } else if (pratio > nearfull_ratio) {
      return NEARFULL;
    }
     return NONE;
  ...
}

bool OSDService::_check_inject_full(DoutPrefixProvider *dpp, s_names type) const
{
  if (injectfull && injectfull_state >= type) {
    // injectfull is either a count of the number of times to return failsafe full
    // or if -1 then always return full
    if (injectfull > 0)
      --injectfull;
    ldpp_dout(dpp, 10) << __func__ << " Injected " << get_full_state_name(type) << " OSD ("
             << (injectfull < 0 ? "set" : std::to_string(injectfull)) << ")"
             << dendl;
    return true;
  }
  return false;
}
```



